{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtmW8N-hUGaD"
      },
      "source": [
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxWFVBWfTvJx",
        "outputId": "16ed6986-1efe-4b7f-a736-0b14ef49ee12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\mrmhm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from __future__ import print_function, division\n",
        "from future.utils import iteritems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwR5RmEeURp-"
      },
      "source": [
        "# 2. Prepare Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUws4vsuUQOt"
      },
      "outputs": [],
      "source": [
        "# Creating lemmatizer instance\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define stopwords\n",
        "stopwords = set(w.rstrip() for w in ('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/refs/heads/master/nlp_class/stopwords.txt'))\n",
        "\n",
        "# Define Positive Reviews\n",
        "positive_reviews = BeautifulSoup(open(r'dataset/sorted_data_acl/electronics/positive.review').read())\n",
        "positive_reviews = positive_reviews.find_all('review_text')\n",
        "\n",
        "# Define negative review\n",
        "negative_reviews = BeautifulSoup(open(r'dataset/sorted_data_acl/electronics/negative.review').read())\n",
        "negative_reviews = negative_reviews.find_all('review_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZwFmgfVXneN"
      },
      "outputs": [],
      "source": [
        "# Shuffling Data\n",
        "np.random.shuffle(positive_reviews)\n",
        "positive_reviews = positive_reviews[:len(negative_reviews)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD3pwLNHZIre"
      },
      "outputs": [],
      "source": [
        "def tokenizer(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(s)\n",
        "    tokens = [t for t in tokens if len(t) > 2]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "5NdY5tl8X_yp",
        "outputId": "d24c2800-91c0-4131-831a-ec326f7ed8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(word_index_map): 11249\n"
          ]
        }
      ],
      "source": [
        "word_index_map = {}\n",
        "current_index = 0\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "orig_reviews = []\n",
        "\n",
        "# Tokenization for positive review\n",
        "for review in positive_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = tokenizer(review.text)\n",
        "    # print(tokens)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "# Tokenization for negative reviews\n",
        "for review in negative_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = tokenizer(review.text)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "print(\"len(word_index_map):\", len(word_index_map))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94-0gv8KZrvm"
      },
      "outputs": [],
      "source": [
        "# Convert tokens to vectors\n",
        "def tokens_to_vectors(tokens, label):\n",
        "    x = np.zeros(len(word_index_map) + 1)\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t] # Get index\n",
        "        x[i] += 1\n",
        "    x = x / x.sum()\n",
        "    x[-1] = label\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97hVeO9rUNRA"
      },
      "outputs": [],
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized) # Total size\n",
        "data = np.zeros((N, len(word_index_map) + 1))\n",
        "i = 0 # Counter\n",
        "\n",
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vectors(tokens, 1)\n",
        "    data[i,:] = xy\n",
        "    i += 1\n",
        "\n",
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vectors(tokens, 0)\n",
        "    data[i,:] = xy\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGAzUntFUNRA"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KagI45dOUNRB"
      },
      "outputs": [],
      "source": [
        "# Shuffle data\n",
        "np.random.shuffle(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAf5m6_aUNRB"
      },
      "outputs": [],
      "source": [
        "# Split X, y\n",
        "\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auhjBvApUNRB"
      },
      "outputs": [],
      "source": [
        "# last 100 rows will be test\n",
        "X_train = X[:-100,]\n",
        "y_train = y[:-100,]\n",
        "X_test = X[-100:,]\n",
        "y_test = y[-100:,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edX-1Q-qUNRB",
        "outputId": "a0bc96cb-2b4a-49c0-ac75-d194393ab4f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification RatE:  0.71\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Score\n",
        "print(\"Classification RatE: \", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72UYrSejUNRC",
        "outputId": "8c9bc85d-31fd-41dc-cc3f-65e7bf11150d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "and 1.488517233432227\n",
            "will -0.6662897273680768\n",
            "cable 0.5711923763996952\n",
            "for 1.9417257380884105\n",
            "that -0.6421632129921748\n",
            "are 1.0110388370109558\n",
            "the -0.743824292784933\n",
            "used 0.6663702371082781\n",
            "month -0.5177913408473829\n",
            "they -0.5397257762919564\n",
            "good 1.4710519178850066\n",
            "sound 0.7546356657879958\n",
            "you 0.8593521783482226\n",
            "n't -1.5187528386267717\n",
            "easy 0.9401641317318584\n",
            "get -0.817692708713708\n",
            "use 1.138059331881418\n",
            "quality 0.9579229588757723\n",
            "but -0.644113150970597\n",
            "best 0.6896325298616318\n",
            "item -0.5983899209160026\n",
            "very 1.0025820911343333\n",
            "well 0.644135280463187\n",
            "with 1.0656965531267084\n",
            "out -0.7397697272660526\n",
            "wa -0.9752808015840501\n",
            "perfect 0.5914611716105562\n",
            "fast 0.5372774836698725\n",
            "have 0.5611432498519805\n",
            "price 1.6712771540090017\n",
            "great 2.7391949938314886\n",
            "money -0.6055016664281294\n",
            "memory 0.5409267841390364\n",
            "buy -0.5712719454550148\n",
            "after -1.173190416121651\n",
            "not -3.1205878918624625\n",
            "doe -0.6285856497408113\n",
            "highly 0.6313710933233982\n",
            "excellent 0.8605200838065036\n",
            "love 0.610432716788226\n",
            "thing -0.6275095449365159\n",
            "did -0.6869444936801729\n",
            "then -0.5767876875488345\n",
            "back -1.0122269741298577\n",
            "speaker 0.6025752966465584\n",
            "return -0.6567728004632881\n",
            "waste -0.5778851129170447\n"
          ]
        }
      ],
      "source": [
        "# Set threshold\n",
        "\n",
        "# let's look at the weights for each word\n",
        "# try it with different threshold values!\n",
        "threshold = 0.5\n",
        "for word, index in iteritems(word_index_map):\n",
        "    weight = model.coef_[0][index]\n",
        "    if weight > threshold or weight < -threshold:\n",
        "        print(word, weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC7_G484UNRC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}